---
title: "Profiling EV users"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, error = F, fig.width = 10)

library(evprof)
library(dplyr)
library(lubridate)
library(ggplot2)
library(purrr)
library(tidyr)
```


For practicing purposes, a data set of charging sessions from the Dutch city of Arnhem has been provided in `data/ev_sessions.RDS`. Read it using function `readRDS`:

```{r read sessions}

```



# Data exploratory analysis

Answer this questions about the data set:

* Which variables does this data set have?
* How many sessions?
* From which time period?


## Data set visualization

Make a line plot with `ggplot` function of the number of sessions per day:

```{r sessions per day}
# 
```

Plot the sessions with function `plot_points`:

```{r exploration-points}
# 
```

Which is the proper `start` hour for this data set? Configure it as a global option:

```{r}
# options(
#   evprof.start.hour = 
# )
```


## Statistic analysis

Use function `summarise_sessions` to see the **median** values of variables `Power`, `Energy`, `ConnectionHours` and `ChargingHours` in our charging sessions data set:

```{r statistic summary}
# 
```

Answer the following questions:

* Do you think the average charging power is high or low?
* Do you think the average energy demand is high or low?
* Do you think the average connection time is high or low?
* Do you think the average charging time is high or low?


A more depth overview of the data set in this sense can be done with a distribution plot (histogram) of each one of these features. Make a histogram grid plot using function `plot_histogram_grid`:

```{r histograms}
# 
```

Answer the following questions:

* Is there any feature that fits a Gaussian Distribution?
* Are there multiple peaks for a same variable?


Try to apply a logarithmic transformation to the most interesting variables and check if the density curves show better bell-curves (Gaussian distributions):

```{r log transformation}
# 
```



# Data preprocessing

The clustering method used in package `{evprof}` is Gaussian Mixture Models with Expectation-Maximization algorithm. To obtain a better performance in GMM clustering it is important to divide the data in smaller groups and clean the outliers, since the different density distributions will result more accentuated and easy to model.

## Divide the data

The division is performed in two steps:

1.  Disconnection day
2.  Time-cycle behaviors


### Division by Disconnection day

As you have seen in the scatter plot of your data set (`plot_points`) you have different groups of sessions showing several "clouds" of points in the plot. Find which hour corresponds to the division between the different groups using function `plot_division_lines`, and classify the sessions between these groups using function `divide_by_disconnection`:

```{r plot division lines}
# 
```

```{r divide by disconnection day}
# 
```

What has changed in the data set?

Which is the percentage of sessions that stay more than 1 day connected? To answer this, first calculate the percentage of sessions belonging to every group of the `Disconnection` column:

```{r distribution disconnection day}
# 
```



### Time-cycle division

It is also important to consider the time-cycles or periods when users change their behaviors. The function `plot_density_2D` lets to analyze the different density of sessions (i.e. users behavior) according to different weekday, month or year. Plot the 2D density plot making use of this function by day of the week:

```{r density 2D plot wday}
# 
```

Do you see a significant difference in the sessions' distribution between days of the week?


Now repeat the plot by month of the year:

```{r density 2D plot month}
# 
```

Do you see a significant difference in the sessions' distribution between months of the year?


Ennumerate the time-cycles that would make more sense to consider:

- 
- 
- 
- 
- ...


Divide the data set by time-cycle sessions using the function `divide_by_timecycle()`, which adds an extra column `Timecycle` to the sessions' data set with the number of time-cycle according to function parameters `months_cycles` and `wdays_cycles`:

```{r divide by time cycle}
# 
```


### Divided data set

Once the two division approaches are applied, our data set counts with two extra columns (`Disconnection` and `Timecycle`) to classify each data point within each group. These columns have integer values corresponding to each division. For a more readable data set we can change these integer values by character strings with the definition of each group, and convert them to `factors` to set a specific order of the levels.

Create a new data set called `sessions_divided` by labelling the different groups from variables `Disconnection` and `Timecycle` according to your preferences, using function `plyr::mapvalues` and `factor` to sort the labels with the `levels` parameter.

```{r divisions}
# sessions_divided <- 
```


## Outliers cleaning

As explained, the clustering method used in package `{evprof}` is Gaussian Mixture Models clustering. This method is sensible to outliers since it tries to explain as most as possible all the variance of the data. To identifying outliers use functions `detect_outliers` and `plot_outliers` and to clean the data before clustering use function `drop_outliers`.

It is recommended to perform the clustering process in a logarithmic scale to reduce data points' sparsity and to avoid limiting the clusters to just the positive domain. The logarithmic transformation can be done in a lot of functions setting the `log` parameter to `TRUE`. We can visualize the data of each subset:

```{r divisions density plot}
# plot_points(sessions_divided, size = 0.2, log = T) + 
#   facet_wrap(vars(Timecycle, Disconnection), scales = 'free')
```


In these plots we see that every group has several points that stand out from most of points. These outliers can be detected specifying a **noise threshold** (in percentage) in function `detect_outliers()`. This function adds a logical column `Outlier` to the sessions data set showing if a session whether a session is considered outlier. This classification can be visualized with function `plot_outliers()` which shows the outliers in grey and the noise level (in percentage) in the title of the graph. This `Outlier` extra column can be removed together with outliers with function `drop_outliers()`. Additionally, to simply discard sessions from a certain limit in both axis (i.e connection hours or starting hour), function `cut_sessions()` filters the sessions data set according to the specified minimum and maximum limits of the corresponding axis.

Split the whole data set in `sessions_divided` in multiple sub-sets. Then, clean the outliers with the functions described in previous pharagraph.

```{r}
# 
```


# Clustering process

Function `cluster_sessions` perform a classification of sessions adding an extra column `Cluster` with the corresponding cluster number. However, Gaussian Mixture Models need a predefined number of clusters `k`. Moreover, this function also requires a `seed` in order to define a specific random seed and being able to reproduce specifics clustering results.

## Parameters selection

The Bayesan Information Criterion (BIC) is a common approach to find the optimal number of clusters to consider for the GMM clustering. The BIC penalizes the non-explained variability by the clusters found, so the higher the BIC value, the better the model. Use the function `choose_k_GMM()` to generate a BIC plot and obtain the optimal number of clusters `k`:

```{r BIC plot}
# 
```

After generating a BIC plot for each one of the 8 sub-sets, the selected number of clusters are:

-   
- 
- 
- 
- ...


Then, since Gaussian Mixture Modeling depends on the random seed of the operation, it is recommended to repeat the clustering process several times and observe the variability. If different clusters are obtained in each repetition, it means that there is still too much noise or variance in the data, that a different number of clusters should be selected, or that the data set should be more divided. Function `save_clustering_iterations()` repeats the clustering the number of times specified in the `it` parameter and saves the results in a PDF file. From this file, then we can choose the optimal seed according to the BIC value. Use this function to create a PDF with 6 iterations (`it=6`) for every sub-set:

```{r clustering iteration}
# 
```

For this study case, each data sub-set has been clustered 6 times and the optimal seeds for each sub-set have resulted as follows:

-   
- 
- 
- 
- ...



## Clustering

Finally, cluster each sub-set with function `cluster_sessions()`, specifying the parameters `k` and `seed` with the corresponding values previously found:

```{r clustering}
# 
```

The object returned by `cluster_sessions()` function is a list with two other objects:

-   `sessions`: a `tibble` containing the sessions' data set with an extra column `Cluster` (i.e. the corresponding cluster number)
-   `models`: a `tibble` with the means and co-variance matrix of each cluster's Gaussian Mixture Models

These two objects correspond to the parameters `sessions` and `models` of the function `plot_bivarGMM()`, which plots each cluster as an ellipse over the sessions' points. Create the plot of every clustered sub-set.

```{r clusters plots}
# 
```




# Profiling

Clusters obtained from GMM don't give a lot of information themselves and separately may have an unclear meaning. In this section we will define each cluster to give them a meaning and relate them to generic user behaviors, i.e. user profiles. Moreover, not every clusters must correspond to a single user profile, clusters with the same or a similar meaning can be grouped to a user profile. Thus, the combination of these multiple Gaussian Mixture Models into a single user profile is what we expect to result in a daily generic behavior of EV users.

As a tool to define the different clusters, function `define_clusters()` prints the average value of the connection start time and connection duration (i.e. the centroid) of each cluster. If the cluster process was performed in logarithmic scale, these values are transformed to natural scale for a better understanding. Moreover, we can pass to the parameters `interpretations` and `profile_names` a list of character strings with the corresponding interpretation of the centroids (e.g. "Connection after work-time, leaving always next morning") and the user profile name assigned to each interpretation (e.g. "Commuter").

For every sub-set of sessions (time-cycle and disconnection day):

1. Visualize the clusters
2. Get the average connection start time and duration with funcion `define_clusters` (without any parameter)
3. Create a vector with the `interpretations` of every cluster 
4. Create a vector with the corresponding user `profile_names`
3. Create the table of clusters' definition with function `define_clusters`, now using parameters `interpretations` and `profile_names`



# Sessions classification into user profiles

After assigning a user profile to each cluster through the clusters definitions with function `define_clusters()`, we can use the data frame that this function outputs as the `clusters_definition` parameter of function `set_profiles()`. This function wraps all sub-sets sessions and clusters definitions to return a total sessions data set with an extra `Profile` column, finishing with this function the user profile classification of the charging sessions data set. The other parameter that function `set_profiles()` needs is the `sessions_clustered`, which is the `sessions` object from the output of `cluster_sessions` function.

Use function `set_profiles` to classify the clustered sessions (obtained from function `cluster_sessions`) according to the clusters definition (obtained from function `define_clusters`). 

```{r set profiles}
# sessions_profiles <-
```

To visualize the classification, plot the charging sessions points with a different color for every user profile, and separate between time-cycle.

```{r plot classification profiles}
# 
```

Describe the main characteristics of every user profile, and explain if a user profile is different between time-cycles or not:

- 
- 
- 
- 
- ...



# User profiles modelling

Each time-cycle has its own **connection models** (connection start time and duration) based on the Gaussian Mixture Models corresponding to every user profile.  However, to estimate new charging sessions we need to **model the energy required** as well. The energy consumed is very sensitive to latest models of EV in the market, since the batteries and charging rates are larger with every new model. To deal with this variability, it is recommended to build energy models using the most recent data in the data set. Moreover, the energy models should be done separately according to the `Power` charging rate, since this is a determinant variable for the value of energy charged. 

Each time-cycle model will be stored in `tibbles` with the following variables:

-   `profile`: Character vector with profiles names. Each profile is a row in the tibble.
-   `profile_ratio`: Numeric vector with the ratio or percentage of sessions corresponding to each profile for this time cycle, obtained from function `get_connection_models()`. In case of modifying these ratios, their values must be always between 0 and 1.
-   `connection_models`: List of tibbles containing the connection models of each profile, obtained from function `get_connection_models()`.
-   `energy_models`: List of tibbles containing the energy models of each profile, obtained from function `get_energy_models()`.


On one hand, after obtaining the connection models (i.e. bi-variate Gaussian Mixture Models weights, means and co-variance matrices) with function `get_connection_models()`, we can visualize them with function `plot_model_clusters()` which outputs a similar ellipses plot than function `plot_bivarGMM()` but using a different color for each user profile instead of clusters (the clusters of a same profile have the same color now). 

On the other hand, the energy models (i.e. uni-variate Gaussian Mixture Models weights, means and variance) obtained with function `get_energy_models()` can be visualized with function `plot_energy_models_density()`. We will create an energy model for every different charging rate in `Power` column of the data set. Therefore, to avoid overfitting, the `Power` value of all sessions has to be rounded to the three most common values in the charging infrastructure: 3.7 kW, 7.4 kW and 11 kW.

Round the charging power of the sessions using function `round_to_interval`:

```{r sessions_energy}
# 
```


## Create the models

For every time-cycle, obtain:

1. The connection models with function `get_connection_models` (and plot them with function `plot_model_clusters`)
2. The energy models with function `get_energy_models` (and plot them with function `plot_energy_models`)

    2.1. Using all sessions data set and using just the last 6 months of sessions. Do you see any difference in the parameters or plots?
    
    2.2. Using `log = TRUE` and `log = FALSE`. What do you think is best?
    
    2.3. Using `by_power = TRUE` and `by_power = FALSE`. Do you think is best?




## Save the EV model

The package `{evprof}` proposes a standard format for the EV model with the function `get_ev_model()`. This function returns an object of class `evmodel`, which can be used then as input of `{evsim}` package to simulate new sessions. The function requires the following variables:

-   `names`: Character vector with the names of the time cycle
-   `months_lst`: List of numeric vectors, each observation containing the months of validity of each time cycle
-   `wdays_lst`: List of numeric vectors, each observation containing the weekdays of validity (week start = 1) of each time cycle
-   `connection_GMM`: List of the connection models returned by function `get_connection_models()`
-   `energy_GMM`List of the connection models returned by function `get_energy_models()`
-   `connection_log`: logical `TRUE` since we have built the connection models in a logarithmic scale
-   `energy_log`: logical `TRUE` since we have built the energy models in a logarithmic scale
-   `tzone`: In our case the sessions are in `Europe/Amsterdam` time-zone


Create the model:

```{r total model}
# 
```

Save the evmodel object to a JSON file using function `save_ev_model`:

```{r save model}
# 
```



